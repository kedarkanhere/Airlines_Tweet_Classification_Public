# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19W4FoZaHotTHdwqxXEmQRNceV1NXSq4D
"""

import numpy as np 
import pandas as pd
import re
import gc
import os
import fileinput
import string
import tensorflow as tf
import zipfile
import datetime
import sys
from tqdm  import tqdm
tqdm.pandas()
from nltk.tokenize import wordpunct_tokenize
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score, roc_auc_score
import pandas
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
from sklearn.metrics import classification_report
import spacy
import matplotlib.pyplot as plt
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

nlp = spacy.load("en_core_web_sm")
prefixes = ('\\n', ) + nlp.Defaults.prefixes
stops = nlp.Defaults.stop_words

df=pd.read_csv('/content/Usecase3_Dataset.csv')

def normalize(comment, lowercase, remove_stopwords):
    if lowercase:
        comment = comment.lower()
        comment= re.sub(r'@\w+',' ',comment)
        comment=re.sub(r'#',' ',comment)
        comment = re.sub('http\S+',' ',comment)
        comment = re.sub(r"[^A-Za-z0-9']+",' ',comment)
        comment = re.sub(r"\s+",' ',comment)
        

        
    comment = nlp(comment)
    lemmatized = list()
    for word in comment:
        lemma = word.lemma_.strip()
        if lemma:
            if not remove_stopwords or (remove_stopwords and lemma not in stops and lemma != '-PRON-'):
                lemmatized.append(lemma)
    return " ".join(lemmatized)

df['Clean_Text']=df.text.apply(normalize,lowercase=True, remove_stopwords=True)

df['Clean_Text'].head(25)

Y = pd.get_dummies(df['airline_sentiment']).values
X=df["Clean_Text"].values

#y_raw=df.to_numpy()
X_train, X_test, y_train, y_test = train_test_split(df["Clean_Text"].values,Y, test_size=0.2, random_state=87,stratify=Y)

vocabulary_size = 4000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(X_train)
sequences = tokenizer.texts_to_sequences(X_train)
X_train = pad_sequences(sequences, maxlen=100)

sequences = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(sequences, maxlen=100)

import keras
keras.backend.clear_session()

model = Sequential()
model.add(Embedding(vocabulary_size, 10, input_length=100))
model.add(Conv1D(64, 3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.4))
model.add(LSTM(100))
#model.add(Flatten())
model.add(Dense(3, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])

stopping_rounds=EarlyStopping(monitor='val_loss', mode='max', verbose=1, patience=2)

hist=model.fit(X_train, y_train,
                    batch_size=128,
                    epochs=8,
                    verbose=1,
                    validation_split=0.1,
                    callbacks=[stopping_rounds],
                    shuffle=True
          )

preds = model.predict(X_test)

print(classification_report(np.argmax(y_test,axis=1),np.argmax(preds,axis=1)))

acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

# Get the amount of epochs for visualization
stopped_epoch = stopping_rounds.stopped_epoch
n_epochs = range(stopped_epoch+1)

# Plot training and validation accuracy
plt.figure(figsize=(15,5))
plt.plot(n_epochs, acc)
plt.plot(n_epochs, val_acc)
plt.title('Accuracy over epochs', weight='bold', fontsize=22)
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Accuracy', fontsize=16)
plt.legend(['Training accuracy', 'Validation accuracy'], fontsize=16)
plt.show()

